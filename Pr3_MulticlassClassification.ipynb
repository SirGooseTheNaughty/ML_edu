{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pr3_MulticlassClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdIerZoZFy2M"
      },
      "source": [
        "# Импорт необходимых модулей \n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Настройки для визуализации\n",
        "# Если используется темная тема - лучше текст сделать белым\n",
        "TEXT_COLOR = 'black'\n",
        "\n",
        "matplotlib.rcParams['figure.figsize'] = (15, 10)\n",
        "matplotlib.rcParams['text.color'] = 'black'\n",
        "matplotlib.rcParams['font.size'] = 14\n",
        "matplotlib.rcParams['axes.labelcolor'] = TEXT_COLOR\n",
        "matplotlib.rcParams['xtick.color'] = TEXT_COLOR\n",
        "matplotlib.rcParams['ytick.color'] = TEXT_COLOR\n",
        "\n",
        "# Зафиксируем состояние случайных чисел\n",
        "RANDOM_STATE = 0\n",
        "np.random.seed(RANDOM_STATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8B55CpuF6p1"
      },
      "source": [
        "# Мультиклассовая классификация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzPFUGgx2WpC"
      },
      "source": [
        "# Классификация классификаций\n",
        "\n",
        "В задаче классификации выделяют несколько типов:\n",
        "\n",
        "- Бинарная - классификация по принципу один из двух возможных: да/нет, правда/ложь, пойдет дождь/будет солнечно и т.д.;\n",
        "- Мультиименная (**One vs Rest**) - навешиваем ярлыки (лэйблы, тэги), предсказание может содержать высокую уверенность в нескольких классах сразу;\n",
        "- Многоклассовая (Мультиноминальная / **Multinominal**) - присваивается один (единственный) из доступных классов;\n",
        "\n",
        "Случаи бинарной и мультиименной классификаций мы уже рассматривали. Мультиклассовая классификация отличается от ранее расмотренных, так как ранее при наличии множества классов мы могли поработать с каждым классом практически по-отдельности, то здесь накладывается ограничение: результатом может быть единственный класс из доступных.\n",
        "\n",
        "Для примера, на вход модели подается рукописная цифра от нуля до девяти, задача модели - классифицировать, какая из цифр нарисована на рисунке.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SxAWxnipicF"
      },
      "source": [
        "Теперь создадим набор данных для работы с ними:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Y846T-Y6Zu"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X_data, y_data = make_classification(\n",
        "    n_samples=210,\n",
        "    n_features=2, \n",
        "    n_redundant=0,\n",
        "    n_informative=2, \n",
        "    n_clusters_per_class=1,\n",
        "    n_classes=3,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "pnts_scatter = plt.scatter(X_data[:, 0], X_data[:, 1], marker='o', c=y_data, s=50, edgecolor='k', )\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.grid(True)\n",
        "plt.legend(handles=pnts_scatter.legend_elements()[0], labels=['0', '1', '2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmv0U01xGFIo"
      },
      "source": [
        "# Функция предказания"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8WRuB-RHRi5"
      },
      "source": [
        "## Формирование предсказания в виде вектора"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqWTj3WuHVQS"
      },
      "source": [
        "В бинарной классификации мы предсказывали скаляры, либо значение уверенности присвоения положительному классу для конкретной записи в данных, либо значение индекса класса (0 или 1 ~ отрицательный или положительный случай).\n",
        "\n",
        "В случае, например, трех классов, задача усложняется, так как предсказанием модели для одной записи должен быть вектор, состоящий из трех элементов, каждый из которых отражает уверенность присвоения данному классу:\n",
        "$$\n",
        "h_W(x^{(i)}) = [0.15, 0.8, 0.05]\n",
        "$$\n",
        "\n",
        "Мы помним, что $X$ - матрица данных (меняться она никак не должна), а вот веса ранее являлись вектором. Так как веса относятся к модели, которую мы применяем, то они могут поменяться и теперь (для мультиклассовой классификации) они представляют собой не вектор, а матрицу:\n",
        "$$\n",
        "W = \n",
        "\\begin{bmatrix}\n",
        "w^{[1]}_0 & w^{[2]}_0 & \\dots & w^{[c]}_0 \\\\\n",
        "w^{[1]}_1 & w^{[2]}_1 & \\dots & w^{[c]}_1 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots & \\\\\n",
        "w^{[1]}_m & w^{[2]}_m & \\dots & w^{[c]}_m \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Матрица имеет размерность $(m, c)$, где $m$ - количество признаков (+1 для логистической регрессии), $c$ - количество предсказываемых классов. \n",
        "\n",
        "> Можно рассматривать такое представление как $c$ линейных моделей.\n",
        "\n",
        "Таким образом, результат линейного умножения в матричном виде должен быть представлен как:\n",
        "$$\n",
        "z(X) = XW =\n",
        "\\begin{bmatrix}\n",
        "z^{(1)[1]} & z^{(1)[2]} & \\dots & z^{(1)[c]} \\\\\n",
        "z^{(2)[1]} & z^{(2)[2]} & \\dots & z^{(2)[c]} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots & \\\\\n",
        "z^{(n)[1]} & z^{(n)[2]} & \\dots & z^{(n)[c]} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "> Линейная часть регрессии так и остается в виде $z(X) = XW$.\n",
        "\n",
        "Размерность $(n, c)$, $n$ - количество записей в данных.\n",
        "\n",
        "При этом истинные значения все также располагаются в векторе $y$ размером $(n, 1)$, и каждый элемент может иметь значения в диапазоне $[0; c)$ (индекс класса).\n",
        "\n",
        "Как же тогда соединить показатели уверенности, которые располагаются в результате предсказания $z(XW)$ с размером $(n, c)$, и вектор истинных значений для сравнения $(n, 1)$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfNdUhF1GAWl"
      },
      "source": [
        "\n",
        "Для выполнения ограничения задачи вводится дополнительная функция **Softmax** вместо функции сигмоиды:\n",
        "$$\n",
        "\\sigma^{[k]}(z)=\\frac{e^{z^{[k]}}}{\\sum_{c}e^{z^{[c]}}}\n",
        "$$ \n",
        "\n",
        "где $c$ - количество предсказываемых классов, $k$ - индекс обрабатываемого элемента в векторе линейного предсказания.\n",
        "\n",
        "Эту функцию мы применяем к каждой строке матрицы $z(X)$ и на выходе получаем матрицу такого же размера, но уже **сумма элементов в кажной строке равна единице**.\n",
        "\n",
        "> Допустим, результатом функции $z(x^{(i)})$ (это строка в матрице $z(X)$) для $i$-й записи данных является вектор $[4.5, 0.8, 6.4]$, тогда выходом функции Softmax является вектор $[0.13, 0.003, 0.867]$. Такое представление говорит о том, что модель на 13% уверена, что запись принадлежит классу 0, на 0.3% - классу 1 и на 86.7% - классу 2.\n",
        "\n",
        "После обработки вектора с помощью функции Softmax выполняется **получение индекса элемента вектора с наибольшим значением**, что и является результатом предсказания функции: $[0.13, 0.003, 0.867] \\rightarrow 2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKwaGsOLTQms"
      },
      "source": [
        "Таким образом, после применения функции Softmax и определения индекса максимального значения в векторе для каждой записи мы получаем вектор предсказаний, соответствующий по формату вектору истинных значений.\n",
        "\n",
        "Для примера:\n",
        "```python\n",
        "y_true = [0, 2, 1, 2, 2, 1]\n",
        "y_pred = [1, 2, 2, 1, 0, 1]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v860TlquNGdM"
      },
      "source": [
        "Визуально реализацию задачи в общем виде классификации можно представить следующим образом:\n",
        "\n",
        "![Схема](https://docs.google.com/uc?export=download&id=1MTBiwNND0Pnw9a-wtlfSTf2Fw1djJUW5)\n",
        "\n",
        "На рисунке видно, что модель создает Logits - это сырые степени уверенности. Для случая бинарной классификации по каждой записи предсказывается скалярная степень уверенности. Для мультиименной и мультиноминальной - вектор степеней уверенностей (каждый элемент вектора - уверенность в присвоении класса по индексу).\n",
        "\n",
        "После получения сырых степеней уверенности они нормируются:\n",
        "- Бинарная и мультиименная - каждый элемент результата проходит черех сигмоиду;\n",
        "- Мультиноминальная - каждый вектор уверенностей проходит через Softmax.\n",
        "\n",
        "Этап предсказания заключается в очень простых операциях:\n",
        "- Бинарная и мультиименная - каждый элемент сравниваем с порогом и присваиваем бинарное состояние;\n",
        "- Мультиноминальная - находим индекс максимального элемента -> это и будет индексом класса в качестве предсказания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWhfmDRWUL2V"
      },
      "source": [
        "Настало время реализации данного функционала. Напишем необходимые функции для выполнения предсказания:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtwq6r5QUHmd"
      },
      "source": [
        "# TODO - напишите функции вычисления softmax и предсказания\n",
        "def softmax(z):\n",
        "    return z_norm\n",
        "\n",
        "\n",
        "def predict_proba(X, W):\n",
        "    '''\n",
        "    Предсказание нормированных степеней уверенности\n",
        "    '''\n",
        "    return y_proba\n",
        "\n",
        "\n",
        "def predict(X, W):\n",
        "    '''\n",
        "    Предсказание индексов классов\n",
        "    '''\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwQgqbFIVaOY"
      },
      "source": [
        "# Проверка\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [-3, 4],\n",
        "    [1, 2],\n",
        "    [-1, 1],\n",
        "    [0, 5],\n",
        "    [0, -2],\n",
        "])\n",
        "\n",
        "W = np.array([\n",
        "    [1, 2, 0],\n",
        "    [0, 2, 1],\n",
        "    [2, 2, 0]\n",
        "])\n",
        "\n",
        "y_prob = predict_proba(X, W)\n",
        "y_pred = predict(X, W)\n",
        "\n",
        "y_pred_true = np.array([1, 0, 1, 0, 1, 2])\n",
        "\n",
        "assert np.all(y_pred == y_pred_true)\n",
        "assert np.all(np.abs(np.sum(y_prob, axis=1)-np.ones(X.shape[0])) < 1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC2pqXyoYIMJ"
      },
      "source": [
        "# Метрики"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBf8BaTWRzOn"
      },
      "source": [
        "## Матрица ошибок для многих классов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS-GyEwjR28V"
      },
      "source": [
        "Раньше мы рассмотрели представление матрицы ошибок: \n",
        "<table>\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <th></th>\n",
        "            <th colspan=2>Предсказание</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td rowspan=3>Истинное</td>\n",
        "            <td></td>\n",
        "            <td>0</td>\n",
        "            <td>1</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>0</td>\n",
        "            <td>TN</td>\n",
        "            <td>FP</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>1</td>\n",
        "            <td>FN</td>\n",
        "            <td>TP</td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP4ljrMHR7qK"
      },
      "source": [
        "Такое представление матрицы удобно для задач бинарной классификации, так как у нас всего два класса. Но лучше запомнить правило распределения для многих классов, чтобы потом было проще воспринимать. $TP$ для класса $k$ - это элемент в матрице по индексам $[k, k]$ (то есть на диагонали в ряду и колонке $k$). Все остальные элементы на диагонали - это $TN$ для этого класса $k$. Аналогично для класса $k$ все элементы в ряду ($[k, :]$) кроме диагонального элемента являются $FN$, а все элементы в колонке ($[:, k]$) кроме диагонального элемента являются $FP$. Вот так постараемся описать общий вид:\n",
        "\n",
        "<table>\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <th></th>\n",
        "            <th colspan=2>Предсказание</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td rowspan=4>Истинное</td>\n",
        "            <td></td>\n",
        "            <td>0</td>\n",
        "            <td>1</td>\n",
        "            <td>k</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>0</td>\n",
        "            <td>$TP_0$</td>\n",
        "            <td>$FP_1$ / $FN_0$</td>\n",
        "            <td>$FP_k$ / $FN_0$</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>1</td>\n",
        "            <td>$FP_0$ / $FN_1$</td>\n",
        "            <td>$TP_1$</td>\n",
        "            <td>$FP_k$ / $FN_1$</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>k</td>\n",
        "            <td>$FP_0$ / $FN_k$</td>\n",
        "            <td>$FP_1$ / $FN_k$</td>\n",
        "            <td>$TP_k$</td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>\n",
        "\n",
        "Расшифровывается это следующим образом:\n",
        "- Для класса 0, по индексу $[0, 0]$ это $TP$, все остальные элементы на диагонали - $TN$, все остальные элементы в ряду - $FN$ (видите индекс снизу), все остальные элементы в колонке - $FP$ (тоже индекс 0).\n",
        "- Для класса $k$, по индексу $[k, k]$ это $TP$, все остальные элементы на диагонали - $TN$, все остальные элементы в ряду - $FN$, все остальные элементы в колонке - $FP$.\n",
        "\n",
        "Такое представление немного сложнее четырех элементов, но оно универсальнее и проще в реализации в коде!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6SXId64YKAj"
      },
      "source": [
        "## Задание\n",
        "\n",
        "С учетом ранее изученных методов визуальной и численной оценки классификатора произведите разделение данных на выборку обучения/тестирования и проведите оценку работы классификатора на данных со случайно заданными весами:\n",
        "\n",
        "> Для разделения воспользуйте функцией `sklearn.model_selection.train_test_split` из пакета `sklearn` и обратите внимание на аргумент `stratify`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltDxf-1rY0Tu"
      },
      "source": [
        "rand_W = np.array([\n",
        "    [-1, -2, -2],\n",
        "    [-7, 2, -3],\n",
        "    [-1, 1, 4],\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7k9grEsobyd"
      },
      "source": [
        "# TODO - произведите разделение данных\n",
        "# NOTE - не забудьте зафиксировать RANDOM_STATE\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErQWb6KItN-a"
      },
      "source": [
        "# Проверка\n",
        "assert np.all([\n",
        "    X_train.shape[0] == 147, \n",
        "    X_test.shape[1] == X_data.shape[1], \n",
        "    y_test.shape[0] == 63,\n",
        "    y_test[y_test == 2].shape[0] == 20\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsQOYv5KZfFB"
      },
      "source": [
        "def plot_2d_decision_boundary(X, W, y_true):\n",
        "    # TODO - функция визуализации пространства решений\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_confusion_matrix(y_true, y_pred):\n",
        "    # TODO - функция генерации матрицы ошибок по векторам предсказания/разметки\n",
        "    return conf_mtrx\n",
        "\n",
        "\n",
        "def draw_confusion_matrix(conf_mtrx):\n",
        "    # TODO - функция отображения матрицы ошибок\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# TODO - функции вычисления метрик \n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "    return acc\n",
        "\n",
        "\n",
        "def calculate_recall_precision_f1(y_true, y_pred, class_index):\n",
        "    return recall, precision, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cZK7AEDmFU8"
      },
      "source": [
        "X = X_test\n",
        "y_true = y_test\n",
        "y_pred = predict(X, rand_W)\n",
        "\n",
        "print(f'Accuracy: {calculate_accuracy(y_true, y_pred)}')\n",
        "print(f'Class 0 metrics: {calculate_recall_precision_f1(y_true, y_pred, 0)}')\n",
        "print(f'Class 1 metrics: {calculate_recall_precision_f1(y_true, y_pred, 1)}')\n",
        "print(f'Class 2 metrics: {calculate_recall_precision_f1(y_true, y_pred, 2)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWe1J4Wlv3T"
      },
      "source": [
        "plot_2d_decision_boundary(X, rand_W, y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5h9620rvquE"
      },
      "source": [
        "conf_mtrx = get_confusion_matrix(y_true, y_pred)\n",
        "draw_confusion_matrix(conf_mtrx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LahFBBC8TLlz"
      },
      "source": [
        "Еще раз взглянем на матрицу ошибок. Таким образом удобно представлять работу классификатора даже для случая многих классов, так как такое представление позволяет увидеть, как модель ведет себя.\n",
        "\n",
        "> Одной особенностью матрицы ошибок является то, что сумма каждой строки по стобцам должна давать точно такое количество значений класса, сколько есть в векторе истинных значений. Проверьте, для класса 2 (желтые кружки) мы получаем $10+10=20$ записей, можно подсчитать в векторе `y_true`, что количество действительно равно 20. *Эта особенность является хорошей проверкой на корректность работы кода и формирования результатов.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5csOzJMeGCtw"
      },
      "source": [
        "# Функция потерь"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBx9aiKsXsI7"
      },
      "source": [
        "## One-Hot кодирование (One-Hot Encoding ~ OHE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ps-jmGEX3Kh"
      },
      "source": [
        "Разработанные функции позволяют получить вектор предсказаний и он соотносится с вектором истинных значений. Как пример:\n",
        "```python\n",
        "y_true = [0, 2, 1, 2, 2, 1]\n",
        "y_pred = [1, 2, 2, 1, 0, 1]\n",
        "```\n",
        "\n",
        "С помощью функции потерь мы можем оценить как далеко от истинны находится вектор предсказания. Одним из вариантов является использование $MSE$ для данного случая, но практика показывает, что для мультиноминальной классификации подходит другой вариант функции потерь, так как у $MSE$ расстояние между индексами 0 и 2 будут больше, чем расстояние между 0 и 1, что вносит искажение в смысл, так как классы по факту должны быть равнозначны.\n",
        "\n",
        "Как и ранее производилось сравнение значений в диапазоне $[0; 1]$ с помощью функции $BCE$, можно также произвести преобразование данных для аналогичного сравнения в диапазоне $[0; 1]$. Для этого применяется подход One-Hot Encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x50Kq129ppue"
      },
      "source": [
        "Суть OHE метода заключается в том, что каждый индекс класс в векторе мы заменяем на вектор, состоящий из нулей и единиц. Делается это путем размещения единицы в векторе на место по индексу номера класса. Количество возможных значений (а в нашем случае - это количество предсказываемых классов) определяет длину вектора:\n",
        "- Класс $0 \\rightarrow [1, 0, 0, 0, 0]$\n",
        "- Класс $3 \\rightarrow [0, 0, 0, 1, 0]$\n",
        "\n",
        "> Такой способ кодирования не привязан в функции потерь и используется во многих других методах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wguEdbmHtYAN"
      },
      "source": [
        "# TODO - функция кодирования вектора индексов классов в представление OHE\n",
        "def onehot_encode(y):\n",
        "    # NOTE - для получения размера вектора воспользуйтесь максимальным значением\n",
        "    #   из вектора y\n",
        "    return y_ohe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7Y8j32ytrkM"
      },
      "source": [
        "# Проверка\n",
        "y_ohe = onehot_encode([1, 2, 1, 2, 1, 0])\n",
        "\n",
        "y_ohe_true = np.array([\n",
        "    [0, 1, 0],\n",
        "    [0, 0, 1],\n",
        "    [0, 1, 0],\n",
        "    [0, 0, 1],\n",
        "    [0, 1, 0],\n",
        "    [1, 0, 0]\n",
        "])\n",
        "assert np.all(y_ohe == y_ohe_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD7JsRO6Xsgl"
      },
      "source": [
        "## Категориальная кросс-энтропия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-oG_tSNGByX"
      },
      "source": [
        "В случае мультиноминальной классификации результатом может быть лишь один из предсказываемых классов в векторе. Достигается это за счет использования функции Softmax. При этом вектор истинных значений должен содержать лишь одну единицу.\n",
        "\n",
        "В случае с мультиименной классификацией мы просто сравниваем каждый элемент вектора истины с каждым элементом предсказания и получаем суммарное отклонение (по аналогии с бинарной классификацией), так как значения внутри вектора между собой не связаны. В данном случае функция Softmax выделит одно из значений так, что один элемент вектора будет иметь значение больше других - этим и воспользуемся при разработке функции потерь: \n",
        "$$\n",
        "J=-{\\sum_{k=1}^{K}y^{[k]}*log(\\hat{y}^{[k]})}\n",
        "$$ \n",
        "где $K$ - количество предсказываемых классов, $\\hat{y}$ - нормированный (Softmax) вектор степеней уверенности, $y$ - вектор истинных значений в формате OHE.\n",
        "\n",
        "> Такое представление вычисляет $J$ для одной записи в данных, для полного объема данных требуется найти среднее всех полученных значений $J$.\n",
        "\n",
        "В чем логика? Почему всего одна часть из BCE? Все очень просто - мы проходим по каждому элементу вектора предсказания и, если это целевой класс, то пытаемся максимизировать его значение к единице. А теперь вспомните, какая зависимость в результате Softmax? Правильно, сумма элементов равна единице, так что при максимизации целевого класса к единице мы автоматически сводим остальные элементы вектора предсказания к нулю, как и записано в векторе истины. PROFIT!\n",
        "\n",
        "> Название ей - категориальная кросс-энтропия (Categorical cross-entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kADUrhuK4Xxt"
      },
      "source": [
        "Для примера, если мы имеет три класса, и конкретная 10я запись в данных классифицирована как класс 1 (закодированный истинный вектор $[0, 1, 0]$), а предсказанные нормированные вероятности (после Softmax) $[0.1, 0.6, 0.3]$, то функция потерь будет выглядеть\n",
        "$$\n",
        "J = -(0*log(0.1) + 1*log(0.6) + 0*log(0.3))\n",
        "$$\n",
        "\n",
        "Как видите, закодированное истинное значение работает как простая маска."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvOEcKCH5ybs"
      },
      "source": [
        "# Производная функции потерь"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjAPCZOS507j"
      },
      "source": [
        "В данной практике мы пропустим вывод производной функции потерь и введем сразу представление:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w^{[k]}_{j}} = X^T(\\hat{y}-y)\n",
        "$$\n",
        "\n",
        "где $k$ - индекс класса $[0; K]$, $j$ - индекс признака $[0; M]$. Размерность матрицы градиента $[M, K]$.\n",
        "\n",
        "Хороший пример того, как выводится на примере двух классов представлен здесь: https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqauvruMF8Ht"
      },
      "source": [
        "# TODO - функция потерь и функция получения матрицы производных\n",
        "def cce_loss(y_true, y_pred_proba):\n",
        "    return loss\n",
        "\n",
        "def cce_loss_deriv(X, y_true, y_pred_proba):\n",
        "    return dJ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NA5nOLwz-FD"
      },
      "source": [
        "# Проверка\n",
        "y_true = np.array([0, 1, 2, 3])\n",
        "y_pred_proba = np.array([\n",
        "  [0.98, 0.01, 0.01, 1e-8],\n",
        "  [0.6, 0.1, 1e-8, 0.3],\n",
        "  [0.1, 0.2, 0.4, 0.3],\n",
        "  [0.2, 0.1, 0.1, 0.6],\n",
        "])\n",
        "\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [2, 3, 4],\n",
        "    [8, 9, 0],\n",
        "    [7, 6, 5],\n",
        "])\n",
        "\n",
        "loss = cce_loss(y_true, y_pred_proba)\n",
        "dJ = cce_loss_deriv(X, y_true, y_pred_proba)\n",
        "\n",
        "dJ_true = np.array([\n",
        "    [ 3.38      ,  0.51      , -4.08999998,  0.20000001],\n",
        "    [ 3.86      , -0.28      , -4.77999997,  1.20000002],\n",
        "    [ 3.34      , -3.07      ,  0.53000004, -0.79999997]\n",
        "])\n",
        "\n",
        "assert loss == 0.9374760389879278\n",
        "assert np.all(np.abs(dJ-dJ_true) < 1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOLZjcka5FL9"
      },
      "source": [
        "# Обучение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lnViHkv44wi"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Настало время обучить нашу модель и эта задача переходит вам! Давайте напишем функцию обучения модели. Учтите следующие особенности:\n",
        "- в ходе обучения применяется степень уверенности, а не конечный предсказанный класс;\n",
        "- функция `predict_proba()` принимает на вход исходные данные и сама добавляет колонку, функция `сce_loss_deriv()` более общая и не добавляет колонку единиц."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjea3YWmz-uN"
      },
      "source": [
        "# TODO - функция обучения и функция отображения истории обучения\n",
        "def fit_model(X, y, lr, n_iter):\n",
        "    # Создаем \n",
        "    y_ohe = onehot_encode(y)\n",
        "    K = y_ohe.shape[1]\n",
        "    M = X.shape[1]+1\n",
        "    W = np.zeros((M, K))\n",
        "    loss_history = []\n",
        "    \n",
        "    # TODO - допишите код обучения\n",
        "    return W, loss_history\n",
        "\n",
        "\n",
        "def show_loss(loss_history):\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9d_8NKA-INd"
      },
      "source": [
        "def fit_model(X, y, lr, n_iter):\n",
        "    y_ohe = onehot_encode(y)\n",
        "    K = y_ohe.shape[1]\n",
        "    M = X.shape[1]+1\n",
        "    W = np.zeros((M, K))\n",
        "    X_linreg = np.c_[np.ones(X.shape[0]), X]\n",
        "    loss_history = []\n",
        "    for i_iter in range(n_iter):\n",
        "        y_pred = predict_proba(X, W)\n",
        "        loss = cce_loss(y, y_pred)\n",
        "\n",
        "        dJ = cce_loss_deriv(X_linreg, y, y_pred)\n",
        "        W -= lr*dJ\n",
        "        loss_history.append(loss)\n",
        "    return W, loss_history\n",
        "\n",
        "\n",
        "def show_loss(loss_history):\n",
        "    plt.plot(loss_history)\n",
        "    plt.grid()\n",
        "    plt.title('Loss history')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('$J(X)$')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwpoA7Ae4rUr"
      },
      "source": [
        "trained_W, loss_history = fit_model(\n",
        "    lr=0.01,\n",
        "    n_iter=100,\n",
        "    X=X_train,\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "show_loss(loss_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSAS2VI75NCp"
      },
      "source": [
        "Отлично, если значение функции потерь постепенно снижается в истории, значит все сделано верно, давайте теперь посмотрим, как работает обученная модель.\n",
        "\n",
        "Отобразите метрики, матрицу ошибок и визуализируйте пространство принятия решений:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThAZjSjC4wiL"
      },
      "source": [
        "plot_2d_decision_boundary(X_test, trained_W, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwFCFrsq41ff"
      },
      "source": [
        "X = X_test\n",
        "y_true = y_test\n",
        "y_pred = predict(X, trained_W)\n",
        "\n",
        "print(f'Accuracy: {calculate_accuracy(y_true, y_pred)}')\n",
        "print(f'Class 0 metrics: {calculate_recall_precision_f1(y_true, y_pred, 0)}')\n",
        "print(f'Class 1 metrics: {calculate_recall_precision_f1(y_true, y_pred, 1)}')\n",
        "print(f'Class 2 metrics: {calculate_recall_precision_f1(y_true, y_pred, 2)}')\n",
        "\n",
        "conf_mtrx = get_confusion_matrix(y_true, y_pred)\n",
        "draw_confusion_matrix(conf_mtrx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDkbflAI5lWi"
      },
      "source": [
        "Замечательно! Модель обучена и работает лучше, чем со случайными весами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pCdmxYjtw_d"
      },
      "source": [
        "# Разработка модели с помощью класса"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXEJi1wGt2Jx"
      },
      "source": [
        "Чтож, вот мы и познакомились с базовыми задачами в обучении с учителем и основными моделями для их решения. Но на этом наша практика не заканчивается! Впереди нас ждут больше моделей и подходов, а сейчас мы познакомимся с одним из подходов, которому обязательно нужно научиться, чтобы улучшить читаемость кода и упростить его. Сейчас мы напишем нашу модель логической регрессии в виде класса!\n",
        "\n",
        "> Воу!\n",
        "\n",
        "Суть такого подхода в том, что модель логистической регрессии представляет собой просто веса. Также, модель “может” обучаться и делать предсказания. До этого мы во все функции передавали веса и пользовались отдельными функциями, но по факту все это относится к модели, поэтому мы воспользуемся объектно-ориентированным подходом и реализуем класс логистической регрессии, а потом создадим объект этого класса и воспользуемся его методами. В качестве атрибута класс будет содержать веса, а его методами будут методы предсказания и обучения.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_36CVGjyT6I"
      },
      "source": [
        "# TODO - реализуйте класс логистической регрессии\n",
        "class LogisticRegression:\n",
        "    def __init__(self):\n",
        "        # В начале инициализируем веса как None, что означает \"модель не обучена\"\n",
        "        self.W = None\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Проверяем веса, если они None (через is - так проверяется тип),\n",
        "        #   тогда сообщаем об ошибке, так как надо сначала обучить модель!\n",
        "        if self.W is None:\n",
        "            print('Model is not trained!')\n",
        "            return None\n",
        "        \n",
        "        # TODO - напишите продолжение предсказания\n",
        "        return y_proba\n",
        "        \n",
        "    def predict(self, X):\n",
        "        # Проверяем веса, если они None (через is - так проверяется тип),\n",
        "        #   тогда сообщаем об ошибке, так как надо сначала обучить модель!\n",
        "        if self.W is None:\n",
        "            print('Model is not trained!')\n",
        "            return None\n",
        "\n",
        "        # TODO - напишите продолжение предсказания\n",
        "        return y_pred\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Для начала нужно инициализировать массив весов с размером (M, K)\n",
        "        y_ohe = onehot_encode(y)\n",
        "        K = y_ohe.shape[1]\n",
        "        M = X.shape[1]+1\n",
        "        self.W = np.zeros((M, K))\n",
        "\n",
        "        # TODO - затем произвести обучения\n",
        "        # NOTE - возвращать веса не нужно, они являются атрибутом класса \n",
        "        #   и должны храниться внутри модели"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyy4BZIyz-cI"
      },
      "source": [
        "# Проверим\n",
        "logreg_model = LogisticRegression()\n",
        "logreg_model.fit(\n",
        "    X_train, y_train,\n",
        "    lr=0.01,\n",
        "    n_iter=100\n",
        ")\n",
        "\n",
        "y_pred = logreg_model.predict(X_test)\n",
        "\n",
        "print(f'Accuracy: {calculate_accuracy(y_test, y_pred)}')\n",
        "print(f'Class 0 metrics: {calculate_recall_precision_f1(y_test, y_pred, 0)}')\n",
        "print(f'Class 1 metrics: {calculate_recall_precision_f1(y_test, y_pred, 1)}')\n",
        "print(f'Class 2 metrics: {calculate_recall_precision_f1(y_test, y_pred, 2)}')\n",
        "\n",
        "conf_mtrx = get_confusion_matrix(y_test, y_pred)\n",
        "draw_confusion_matrix(conf_mtrx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe2sWL_70j99"
      },
      "source": [
        "Проверьте себя, метрики и матрица ошибок должны получиться такими же, как и при обучении ранее.\n",
        "\n",
        "> Может вам и не кажется на данный момент код более удобным или читаемым, но представьте, если модель будет содержать не только веса, а еще ворох параметров - передавать их каждый раз в функции!? Практика разработки с использование объектной модели - это очень хорошая практика, так что вы можете в будущем попробовать писать без нее, но если язык позволяет (а Python поддерживает ООП), то это может сильно упростить разработку!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmArkHVP52pN"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Произведите оценку работы модели с использованием кросс-валидации и класса `LogisticRegression`. В качестве метрики для оценки используйте среднюю F1 по всем классам.\n",
        "\n",
        "> Для получения фолдов со стратификацией воспользуйтесь функцией из модуля `sklearn.model_selection.StratifiedKFold`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pce5F66Y6GjF"
      },
      "source": [
        "# TODO - функция оценки работы модели кросс-валидацией с пятью фолдами \n",
        "# NOTE - не забудьте зафиксировать RANDOM_STATE\n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "\n",
        "# NOTE - обратите внимание, в качестве агрумента передаем модель, \n",
        "#   которую будем проверять\n",
        "def cross_val_score_f1(X, y, k_folds, model):\n",
        "    f1_values = []\n",
        "\n",
        "    return np.mean(f1_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiIfLfYS2RV-"
      },
      "source": [
        "# Используем!\n",
        "logreg_model = LogisticRegression()\n",
        "\n",
        "f1_result = cross_val_score_f1(X_data, y_data, k_folds=5, model=logreg_model)\n",
        "print(f1_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qlw8DLfTOkoM"
      },
      "source": [
        "# Выводы - Задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twv1rn_1BL4m"
      },
      "source": [
        "Классическая рубрика - \"Напиши вывод сам\". Не стесняйтесь шевелить извилинами и искать причинно-следственные связи. И как обычно несколько вспомогательных вопросов, чтобы было проще начать. \n",
        "\n",
        "1. В каких случаях необходимо использовать мультиноминальную классификацию? \n",
        "2. Что позволяет оценить функция потерь? И зачем нужна эта оценка? \n",
        "3. Как это \"обучение с учителем\"?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaGLSg6fBb0F"
      },
      "source": [
        "# Вопросики! \n",
        "\n",
        "1. Что используется в мультиноминальной классификации вместо сигмоиды? \n",
        "2. Почему MSE для случае мутиноминальной классификации - не лучший выбор? \n",
        "3. Что такое ОНЕ и где это использовать? \n",
        "4. В чём категориальность категориальной кросс-энтропии? \n",
        "5. Зачем оборачивать модель логистической регрессии в класс? Какие преимущества это даёт? \n"
      ]
    }
  ]
}